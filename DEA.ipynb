{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=6>Jane Street Market Prediction</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 About "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Buy low, sell high.” It sounds so easy….\n",
    "\n",
    "In reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n",
    "\n",
    "In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.\n",
    "\n",
    "Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.\n",
    "\n",
    "In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Challenge Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.\n",
    "\n",
    "In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Organizers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.\n",
    "\n",
    "Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Score\n",
    "\n",
    "Each row in the test set represents a trading opportunity for which we will be predicting an action value, **1 to make the trade and 0 to pass on it**. Each trade j has an associated weight and resp, which represents a return.\n",
    "\n",
    "For each date i, we define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p_i = \\sum_j(weight_{ij} * resp_{ij} * action_{ij}), $$\n",
    "$$ t = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}}, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $|i|$ is the number of unique dates in the test set. The utility is then defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u = min(max(t,0), 6)  \\sum p_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Utility(date, weight, resp, action):\n",
    "    count = len(np.unique(date))\n",
    "    p = np.zeros(count)\n",
    "    \"\"\"\n",
    "    for i, day in enumerate(np.unique(date)):\n",
    "        p[i] = np.sum(weight[date == day] * resp[date == day] * action[date == day])\n",
    "    \"\"\"\n",
    "    p = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(p) / np.sqrt(np.sum(p ** 2)) * np.sqrt(250 / count)\n",
    "    u = np.clip(t, 0, 6) * np.sum(p)\n",
    "    return u "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the following template in Kaggle Notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import janestreet\n",
    "env = janestreet.make_env() # initialize the environment\n",
    "iter_test = env.iter_test() # an iterator which loops over the test set\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    sample_prediction_df.action = 0 #make your 0/1 prediction here\n",
    "    env.predict(sample_prediction_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Code Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this is a code competition, hence it has certain requirements for submission. Submissions to this competition must be made through Notebooks. For this competition, training is not required in Notebooks. In order for to be eligible for submission, the following conditions must be met:\n",
    "\n",
    "### Training Phase\n",
    "- Your notebook must use the time-series module to make predictions\n",
    "- CPU Notebook <= 4 hours run-time\n",
    "- GPU Notebook <= 4 hours run-time\n",
    "- Freely & publicly available external data is allowed, including pre-trained models\n",
    "\n",
    "### Forecasting Phase\n",
    "Because the size of the test set will change during the live forecasting phase, the time limits will be adjusted in proportion to the test set size, with a **10% added time allowance**. As a hypothetical example, if there are <span style=\"color:red\">1,000,000 test rows</span> and a <span style=\"color:red\">4 hour runtime</span> limit during the **training phase** and the **forecasting phase** has <span style=\"color:red\">2,000,000 rows</span>, your notebook will be allowed **8 hours + 10% = 8.8 hours during the forecasting phase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains an anonymized set of features, <span style=\"color:red\">feature_{0...129}</span>, representing real stock market data. \n",
    "1. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: \n",
    "    - 1 to make the trade and \n",
    "    - 0 to pass on it. \n",
    "\n",
    "2. Each trade has an associated <span style=\"color:red\">weight and resp</span>, which together represents a return on the trade. \n",
    "3. The `date column` is an **integer** which represents the day of the trade.\n",
    "4. The `ts_id` represents a time ordering. \n",
    "5. In addition to anonymized feature values, you are provided with metadata about the features in `features.csv`.\n",
    "\n",
    "## 3.1 Train.csv\n",
    "\n",
    "- In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. \n",
    "- **Non-Scored:** Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n",
    "\n",
    "## 3.2 Test Set\n",
    "- During the model training phase of the competition, this unseen test set is comprised of historical data.\n",
    "- During the live forecasting phase, the test set will use periodically updated live market data.\n",
    "- Note that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set.\n",
    "\n",
    "    1. **example_test.csv** - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get/set the test set and predictions.\n",
    "    2. **example_sample_submission.csv** - a mock sample submission file in the correct format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=6>EDA</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().style.background_gradient(cmap=\"gnuplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_train.isna().sum().sort_values(ascending=False).to_frame().reset_index()\n",
    "temp.columns = [\"Cols\", \"NaN-counts\"]\n",
    "\n",
    "def plot_bar(x, y, df):\n",
    "    plt.figure(figsize=(20, 30))\n",
    "    sns.barplot(x=x, y=y, data=df, orient=\"h\")\n",
    "    plt.title(f\"Top {len(df)} {x} {y}\")\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "plot_bar(x='NaN-counts', y='Cols', df=temp.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 500 days of data in train.csv (i.e. two years of trading data). Let us take a look at the cumulative values of resp over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "balance= pd.Series(df_train['resp']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_ylabel (\"Cumulative resp\", fontsize=18);\n",
    "balance.plot(lw=3);\n",
    "del balance\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as four [time horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n",
    "> \"*The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "balance= pd.Series(df['resp']).cumsum()\n",
    "resp_1= pd.Series(df['resp_1']).cumsum()\n",
    "resp_2= pd.Series(df['resp_2']).cumsum()\n",
    "resp_3= pd.Series(df['resp_3']).cumsum()\n",
    "resp_4= pd.Series(df['resp_4']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\n",
    "balance.plot(lw=3)\n",
    "resp_1.plot(lw=3)\n",
    "resp_2.plot(lw=3)\n",
    "resp_3.plot(lw=3)\n",
    "resp_4.plot(lw=3)\n",
    "plt.legend(loc=\"upper left\");\n",
    "del resp_1\n",
    "del resp_2\n",
    "del resp_3\n",
    "del resp_4\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `resp` (in blue) most closely follows time horizon 4 (`resp_4` is the uppermost curve, in purple). \n",
    "\n",
    "In the notebook [\"*Jane Street: time horizons and volatilities*\"](https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities) written by [pcarta](pcarta), if I understand correctly, by using [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) it is calculated that if the time horizon $(T_j$) for `resp_1` (*i.e.* $T_1$) is 1, then \n",
    "* $T_j($ `resp_2` $) ~\\approx 1.4 ~T_1$\n",
    "* $T_j($ `resp_3` $) ~\\approx 3.9 ~T_1$ \n",
    "* $T_j($ `resp_4` $) ~\\approx 11.1 ~T_1$\n",
    "\n",
    "where $T_1$ could correspond to 5 trading days.\n",
    "\n",
    "Let us now plot a histogram of all of the `resp` values (here only shown for values between -0.05 and 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.distplot(df_train['resp'], \n",
    "             bins=3000, \n",
    "             kde_kws={\"clip\":(-0.05,0.05)}, \n",
    "             hist_kws={\"range\":(-0.05,0.05)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of the resp values\", size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution has very long tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_resp = df_train['resp'].min()\n",
    "print('The minimum value for resp is: %.5f' % min_resp)\n",
    "max_resp = df_train['resp'].max()\n",
    "print('The maximum value for resp is:  %.5f' % max_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also calculate the [skew](https://en.wikipedia.org/wiki/Skewness) and [kurtosis](https://en.wikipedia.org/wiki/Kurtosis) of this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skew of resp is:      %.2f\" %df_train['resp'].skew() )\n",
    "print(\"Kurtosis of resp is: %.2f\"  %df_train['resp'].kurtosis() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us fit a Cauchy distribution to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "# the values\n",
    "x = list(range(len(values)))\n",
    "x = [((i)-1500)/30000 for i in x]\n",
    "y = values\n",
    "\n",
    "def Lorentzian(x, x0, gamma, A):\n",
    "    return A * gamma**2/(gamma**2+( x - x0 )**2)\n",
    "\n",
    "# seed guess\n",
    "initial_guess=(0, 0.001, 3000)\n",
    "\n",
    "# the fit\n",
    "parameters,covariance=curve_fit(Lorentzian,x,y,initial_guess)\n",
    "sigma=np.sqrt(np.diag(covariance))\n",
    "\n",
    "# and plot\n",
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.distplot(df_train['resp'], \n",
    "             bins=3000, \n",
    "             kde_kws={\"clip\":(-0.05,0.05)}, \n",
    "             hist_kws={\"range\":(-0.05,0.05)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "#norm = plt.Normalize(values.min(), values.max())\n",
    "#colors = plt.cm.jet(norm(values))\n",
    "#for rec, col in zip(ax.patches, colors):\n",
    "#    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of the resp values\", size=14)\n",
    "plt.plot(x,Lorentzian(x,*parameters),'--',color='black',lw=3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a Cauchy distribution can be generated from the ratio of two independent normally distributed random variables with mean zero. The paper by [David E. Harris \"*The Distribution of Returns*\"](https://www.scirp.org/pdf/JMF_2017083015172459.pdf) goes into detail regarding the use of a Cauchy distribution to model returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Each trade has an associated `weight` and `resp`, which together represents a return on the trade.\n",
    "Trades with `weight = 0` were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_zeros = (100/df_train.shape[0])*((df_train.weight.values == 0).sum())\n",
    "print('Percentage of zero weights is: %i' % percent_zeros +\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if there are any negative weights. A negative weight would be meaningless, but you never know..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_weight = df_train['weight'].min()\n",
    "print('The minimum weight is: %.2f' % min_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weight = df_train['weight'].max()\n",
    "print('The maximum weight was: %.2f' % max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['weight']==df_train['weight'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at a histogram of the non-zero weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.distplot(df_train['weight'], \n",
    "             bins=1400, \n",
    "             kde_kws={\"clip\":(0.001,1.4)}, \n",
    "             hist_kws={\"range\":(0.001,1.4)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of non-zero weights\", size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two peaks, one situated at `weight` $\\approx$ 0.17, and a lower, broader peak at `weight` $\\approx$ 0.34. Could this be indicative of two underlying distributions that we see here, superimposed on each other? Maybe one distribution of weights correspond to selling, and the other to buying?\n",
    "\n",
    "We can plot the logarithm of the weights (*Credit*: [\"*Target Engineering; CV; ⚡ Multi-Target*\"](https://www.kaggle.com/marketneutral/target-engineering-cv-multi-target) by [marketneutral](https://www.kaggle.com/marketneutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_nonZero = df_train.query('weight > 0').reset_index(drop = True)\n",
    "plt.figure(figsize = (10,4))\n",
    "ax = sns.distplot(np.log(train_data_nonZero['weight']), \n",
    "             bins=1000, \n",
    "             kde_kws={\"clip\":(-4,5)}, \n",
    "             hist_kws={\"range\":(-4,5)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\n",
    "plt.show();\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can now try to fit a pair of Gaussian functions to this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "# the values\n",
    "x = list(range(len(values)))\n",
    "x = [(i/110)-4 for i in x]\n",
    "y = values\n",
    "\n",
    "# define a Gaussian function\n",
    "def Gaussian(x,mu,sigma,A):\n",
    "    return A*np.exp(-0.5 * ((x-mu)/sigma)**2)\n",
    "\n",
    "def bimodal(x,mu_1,sigma_1,A_1,mu_2,sigma_2,A_2):\n",
    "    return Gaussian(x,mu_1,sigma_1,A_1) + Gaussian(x,mu_2,sigma_2,A_2)\n",
    "\n",
    "# seed guess\n",
    "initial_guess=(1, 1 , 1,    1, 1, 1)\n",
    "\n",
    "# the fit\n",
    "parameters,covariance=curve_fit(bimodal,x,y,initial_guess)\n",
    "sigma=np.sqrt(np.diag(covariance))\n",
    "\n",
    "# the plot\n",
    "plt.figure(figsize = (10,4))\n",
    "ax = sns.distplot(np.log(train_data_nonZero['weight']), \n",
    "             bins=1000, \n",
    "             kde_kws={\"clip\":(-4,5)}, \n",
    "             hist_kws={\"range\":(-4,5)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of the logarithm of the non-zero weights\", size=14)\n",
    "# plot gaussian #1\n",
    "plt.plot(x,Gaussian(x,parameters[0],parameters[1],parameters[2]),':',color='black',lw=2,label='Gaussian #1', alpha=0.8)\n",
    "# plot gaussian #2\n",
    "plt.plot(x,Gaussian(x,parameters[3],parameters[4],parameters[5]),'--',color='black',lw=2,label='Gaussian #2', alpha=0.8)\n",
    "# plot the two gaussians together\n",
    "plt.plot(x,bimodal(x,*parameters),color='black',lw=2, alpha=0.7)\n",
    "plt.legend(loc=\"upper left\");\n",
    "plt.show();\n",
    "del values\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with limited success; the narrower left hand peak seems to be some other distribution. (Just in case, the  μ  of the small Gaussian is located at -1.32, and the large Gaussian at 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the cumulative daily return over time, which is given by weight multiplied by the value of resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train\n",
    "train_data['weight_resp']   = train_data['weight']*train_data['resp']\n",
    "train_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\n",
    "train_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\n",
    "train_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\n",
    "train_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "resp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\n",
    "resp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\n",
    "resp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\n",
    "resp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\n",
    "resp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\n",
    "resp.plot(lw=3, label='resp x weight')\n",
    "resp_1.plot(lw=3, label='resp_1 x weight')\n",
    "resp_2.plot(lw=3, label='resp_2 x weight')\n",
    "resp_3.plot(lw=3, label='resp_3 x weight')\n",
    "resp_4.plot(lw=3, label='resp_4 x weight')\n",
    "# day 85 marker\n",
    "ax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n",
    "ax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "plt.legend(loc=\"lower left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shortest time horizons, `resp_1`, `resp_2` and `resp_3`, representing a more conservative strategy, result in the lowest return.\n",
    "\n",
    "We shall now plot a histogram of the `weight` multiplied by the value of `resp` (after removing the 0 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_0 = train_data.query('weight > 0').reset_index(drop = True)\n",
    "train_data_no_0['wAbsResp'] = train_data_no_0['weight'] * (train_data_no_0['resp'])\n",
    "#plot\n",
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.distplot(train_data_no_0['wAbsResp'], \n",
    "             bins=1500, \n",
    "             kde_kws={\"clip\":(-0.02,0.02)}, \n",
    "             hist_kws={\"range\":(-0.02,0.02)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of the weights * resp\", size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the number of `ts_id` per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder [did Jane Street modify their trading model around day 85?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930) Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or *vice versa*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_per_day = train_data.groupby(['date'])['ts_id'].count()\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.plot(trades_per_day)\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_title (\"Total number of ts_id for each day\", fontsize=18)\n",
    "# day 85 marker\n",
    "ax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n",
    "ax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_xlim(xmax=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a [trading day](https://en.wikipedia.org/wiki/Trading_day) is 6½ hours long (*i.e.* 23400 seconds) then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.plot(23400/trades_per_day)\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_ylabel (\"Av. time between trades (s)\", fontsize=18)\n",
    "ax.set_title (\"Average time between trades for each day\", fontsize=18)\n",
    "ax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n",
    "ax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_xlim(xmax=500)\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_ylim(ymax=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a histogram of the number of trades per day (it has been [suggested](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930#1125847) that the number of trades per day is an indication of the [volatility](https://www.investopedia.com/terms/v/volatility.asp) that day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "# the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n",
    "# the maximum number of trades per day is 18884\n",
    "# I have used 125 bins for the 500 days\n",
    "ax = sns.distplot(trades_per_day, \n",
    "             bins=125, \n",
    "             kde_kws={\"clip\":(1000,20000)}, \n",
    "             hist_kws={\"range\":(1000,20000)},\n",
    "             color='darkcyan', \n",
    "             kde=True);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Number of trades per day\", size=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that is the case, then 'volitile' days, say with more than 9k trades (*i.e.* `ts_id`) per day, are the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volitile_days = pd.DataFrame(trades_per_day[trades_per_day > 9000])\n",
    "volitile_days.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that almost all of the days having a large volume of trades are before and up to day 85.\n",
    "\n",
    "Also related to time is `feature_64` which seems to be some sort of daily clock as we shall see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_0\n",
    "First of all, `feature_0` seems to be a little unusual, as it is composed solely of the integers `+1` or `-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['feature_0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "feature_0 = pd.Series(train_data['feature_0']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\n",
    "feature_0.plot(lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also very interesting to plot the cumulative `resp` and return (`resp`\\*`weight`) for `feature_0 = +1` and `feature_0 = -1` individually (Credit: [\"*An observation about feature_0*\"](https://www.kaggle.com/c/jane-street-market-prediction/discussion/204963) by [therocket290](https://www.kaggle.com/therocket290))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that \"+1\" and the \"-1\" projections describe very different return dynamics.\n",
    "In the notebook [\"*Feature 0, beyond feature 0*\"](https://www.kaggle.com/nanomathias/feature-0-beyond-feature-0) written by [NanoMathias](https://www.kaggle.com/nanomathias) a [uniform manifold approximation and projection (UMAP)](https://arxiv.org/abs/1802.03426) is performed and shows that `feature_0`  effectively classifies two distributions of features.\n",
    "There have been many suggestions made regarding the nature of this feature on the discussion topic [\"*What is \"feature_0\" ?*\"](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199462) such as `feature_0` representing the direction of the trade or things like bid/ask, long/short, or call/put.\n",
    "\n",
    "One possibility is that `feature_0` represents something similar to the [Lee and Ready 'Tick' model](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1540-6261.1991.tb02683.x) for classifying individual trades as market buy or market sell orders, using intraday trade and quote data.\n",
    "A buy initiated trade is labeled as \"1\", and a sell-initiated trade is labeled as \"-1\" (*Source*: § 19.3.1 of [\"*Advances in Financial Machine Learning*\"](https://www.wiley.com/en-es/Advances+in+Financial+Machine+Learning-p-9781119482109) by Marcos Lopez de Prado)\n",
    "\n",
    "$$\n",
    "b_t = \n",
    "\\begin{cases} \n",
    "  1  & \\mbox{if }\\Delta p_t > 0\\\\\n",
    "  -1 & \\mbox{if }\\Delta p_t < 0\\\\\n",
    "  b_{t-1} & \\mbox{if }\\Delta p_t = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $p_t$ is the price of the trade indexed by $t = 1,\\ldots , T$, and $b_0$ is arbitrarily set to\n",
    "1.\n",
    "\n",
    "If we look at the correlation matrix (see below) it can be seen that there is a strong positive correlation between `feature_0` and the **Tag 12** features, a strong negative correlation with the **Tag 13** features. There is also a negative correlation with the **Tag 25** and **Tag 27** features, and a positive correlation with  the **Tag 24** features.\n",
    "\n",
    "Other than features 37, 38, 39 and 40 all of the above features are `resp` related features (see below) with the strongest correlation being with the `resp_4` features.\n",
    "\n",
    "### feature_{1...129}\n",
    "There seem to be four general 'types' of features, here is a plot of an example of one of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(20,10))\n",
    "\n",
    "ax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\n",
    "ax1.set_title (\"Linear\", fontsize=22);\n",
    "ax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\n",
    "ax1.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax1.set_xlim(xmin=0)\n",
    "ax1.set_ylabel (\"feature_1\", fontsize=18);\n",
    "\n",
    "ax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\n",
    "ax2.set_title (\"Noisy\", fontsize=22);\n",
    "ax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\n",
    "ax2.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax2.set_xlim(xmin=0)\n",
    "ax2.set_ylabel (\"feature_3\", fontsize=18);\n",
    "\n",
    "ax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\n",
    "ax3.set_title (\"Hybryd (Tag 21)\", fontsize=22);\n",
    "ax3.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax3.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\n",
    "ax3.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax3.set_xlim(xmin=0)\n",
    "ax3.set_ylabel (\"feature_55\", fontsize=18);\n",
    "\n",
    "ax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\n",
    "ax4.set_title (\"Negative\", fontsize=22)\n",
    "ax4.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax4.set_ylabel (\"feature_73\", fontsize=18);\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Linear' features\n",
    "* 1 \n",
    "* 7, 9, 11, 13, 15\n",
    "* 17, 19, 21, 23, 25\n",
    "* 18,  20,  22,  24, 26\n",
    "* 27, 29, 21, 33, 35\n",
    "* 28, 30, 32, 34, 36\n",
    "* 84, 85, 86, 87, 88\n",
    "* 90, 91, 92, 93, 94\n",
    "* 96, 97, 98, 99, 100\n",
    "* 102 (strong change in gradient), 103, 104, 105, 106\n",
    "\n",
    "as well as\n",
    "41, 46, 47, 48, 49, 50, 51, 53, 54, 69, 89, 95 (strong change in gradient), 101, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124.\n",
    "### Features 41, 42 and 43 (Tag 14)\n",
    "The **Tag 14** set are interesting as they appear to be \"stratified\"; only adopting discrete values throughout the day (could these be a value of a [security](https://en.wikipedia.org/wiki/Security_(finance%29)?).\n",
    "Here are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the *missing data* section below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_0 = train_data.loc[train_data['date'] == 0]\n",
    "day_1 = train_data.loc[train_data['date'] == 1]\n",
    "day_3 = train_data.loc[train_data['date'] == 3]\n",
    "three_days = pd.concat([day_0, day_1, day_3])\n",
    "three_days.plot.scatter(x='ts_id', y='feature_41', s=0.5, figsize=(15,3));\n",
    "three_days.plot.scatter(x='ts_id', y='feature_42', s=0.5, figsize=(15,3));\n",
    "three_days.plot.scatter(x='ts_id', y='feature_43', s=0.5, figsize=(15,3));\n",
    "#del day_0\n",
    "del day_1\n",
    "del day_3\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three features also have very interesting lag plots, where we plot the value of the feature at `ts_id` $(n)$ with respect to the next value of the feature, *i.e.*  at `ts_id` $(n+1)$, (here for day 0). Red markers have been placed at (0,0) as a visual aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "lag_plot(day_0['feature_41'], lag=1, s=0.5, ax=ax[0])\n",
    "lag_plot(day_0['feature_42'], lag=1, s=0.5, ax=ax[1])\n",
    "lag_plot(day_0['feature_43'], lag=1, s=0.5, ax=ax[2])\n",
    "ax[0].title.set_text('feature_41')\n",
    "ax[0].set_xlabel(\"ts_id (n)\")\n",
    "ax[0].set_ylabel(\"ts_id (n+1)\")\n",
    "ax[1].title.set_text('feature_42')\n",
    "ax[1].set_xlabel(\"ts_id (n)\")\n",
    "ax[1].set_ylabel(\"ts_id (n+1)\")\n",
    "ax[2].title.set_text('feature_43')\n",
    "ax[2].set_xlabel(\"ts_id (n)\")\n",
    "ax[2].set_ylabel(\"ts_id (n+1)\")\n",
    "\n",
    "ax[0].plot(0, 0, 'r.', markersize=15.0)\n",
    "ax[1].plot(0, 0, 'r.', markersize=15.0)\n",
    "ax[2].plot(0, 0, 'r.', markersize=15.0);\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag 18 features: 44 (+ tag 15) and 45 (+ tag 17)\n",
    "These are similar to the Tag 14 features seen above, but are now much more centred around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_days.plot.scatter(x='ts_id', y='feature_44', s=0.5, figsize=(15,3));\n",
    "three_days.plot.scatter(x='ts_id', y='feature_45', s=0.5, figsize=(15,3));\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following lag plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "lag_plot(day_0['feature_44'], lag=1, s=0.5, ax=ax[0])\n",
    "lag_plot(day_0['feature_45'], lag=1, s=0.5, ax=ax[1])\n",
    "ax[0].title.set_text('feature_44')\n",
    "ax[0].set_xlabel(\"ts_id (n)\")\n",
    "ax[0].set_ylabel(\"ts_id (n+1)\")\n",
    "ax[1].title.set_text('feature_45')\n",
    "ax[1].set_xlabel(\"ts_id (n)\")\n",
    "ax[1].set_ylabel(\"ts_id (n+1)\")\n",
    "\n",
    "ax[0].plot(0, 0, 'r.', markersize=15.0)\n",
    "ax[1].plot(0, 0, 'r.', markersize=15.0);\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features 60 to 68 (Tag 22)\n",
    "We have the **Tag 22** set:\n",
    "* 60, 61, 62, 63, 64, 65, 66, 67, 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "feature_60= pd.Series(train_data['feature_60']).cumsum()\n",
    "feature_61= pd.Series(train_data['feature_61']).cumsum()\n",
    "feature_62= pd.Series(train_data['feature_62']).cumsum()\n",
    "feature_63= pd.Series(train_data['feature_63']).cumsum()\n",
    "feature_64= pd.Series(train_data['feature_64']).cumsum()\n",
    "feature_65= pd.Series(train_data['feature_65']).cumsum()\n",
    "feature_66= pd.Series(train_data['feature_66']).cumsum()\n",
    "feature_67= pd.Series(train_data['feature_67']).cumsum()\n",
    "feature_68= pd.Series(train_data['feature_68']).cumsum()\n",
    "#feature_69= pd.Series(train_data['feature_69']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_title (\"Cumulative plot for feature_60 ... feature_68 (Tag 22).\", fontsize=18)\n",
    "feature_60.plot(lw=3)\n",
    "feature_61.plot(lw=3)\n",
    "feature_62.plot(lw=3)\n",
    "feature_63.plot(lw=3)\n",
    "feature_64.plot(lw=3)\n",
    "feature_65.plot(lw=3)\n",
    "feature_66.plot(lw=3)\n",
    "feature_67.plot(lw=3)\n",
    "feature_68.plot(lw=3)\n",
    "#feature_69.plot(lw=3)\n",
    "plt.legend(loc=\"upper left\");\n",
    "del feature_60, feature_61, feature_62, feature_63, feature_64, feature_65, feature_66 ,feature_67, feature_68\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed `feature_60` and `feature_61` (both having Tags 22 & 12) are virtually coincident. \n",
    "The same goes for `feature_62` and `feature_63` (both having Tags 22 & 13), \n",
    "`feature_65` and `feature_66` (both having Tags 22 & 12) and\n",
    "`feature_67` and `feature_68` (both having Tags 22 & 13). Let us plot these features as distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"bright\")\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "sns.distplot(train_data[['feature_60']], hist=True, bins=200,  ax=axes[0,0])\n",
    "sns.distplot(train_data[['feature_61']], hist=True, bins=200,  ax=axes[0,0])\n",
    "axes[0,0].set_title (\"features 60 and 61\", fontsize=18)\n",
    "axes[0,0].legend(labels=['60', '61'])\n",
    "\n",
    "sns.distplot(train_data[['feature_62']], hist=True,  bins=200, ax=axes[0,1])\n",
    "sns.distplot(train_data[['feature_63']], hist=True,  bins=200, ax=axes[0,1])\n",
    "axes[0,1].set_title (\"features 62 and 63\", fontsize=18)\n",
    "axes[0,1].legend(labels=['62', '63'])\n",
    "\n",
    "sns.distplot(train_data[['feature_65']], hist=True,  bins=200, ax=axes[1,0])\n",
    "sns.distplot(train_data[['feature_66']], hist=True,  bins=200, ax=axes[1,0])\n",
    "axes[1,0].set_title (\"features 65 and 66\", fontsize=18)\n",
    "axes[1,0].legend(labels=['65', '66'])\n",
    "\n",
    "\n",
    "sns.distplot(train_data[['feature_67']], hist=True,  bins=200, ax=axes[1,1])\n",
    "sns.distplot(train_data[['feature_68']], hist=True,  bins=200, ax=axes[1,1])\n",
    "axes[1,1].set_title (\"features 67 and 68\", fontsize=18)\n",
    "axes[1,1].legend(labels=['67', '68'])\n",
    "\n",
    "plt.show();\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in between them is `feature_64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,5))\n",
    "ax = sns.distplot(train_data['feature_64'], \n",
    "             bins=1200, \n",
    "             kde_kws={\"clip\":(-6,6)}, \n",
    "             hist_kws={\"range\":(-6,6)},\n",
    "             color='darkcyan', \n",
    "             kde=False);\n",
    "values = np.array([rec.get_height() for rec in ax.patches])\n",
    "norm = plt.Normalize(values.min(), values.max())\n",
    "colors = plt.cm.jet(norm(values))\n",
    "for rec, col in zip(ax.patches, colors):\n",
    "    rec.set_color(col)\n",
    "plt.xlabel(\"Histogram of feature_64\", size=14)\n",
    "plt.show();\n",
    "del values\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which has a big gap for values in the range 0.7 to 1.38. (Incidentally,  $\\ln(2) \\approx 0.693...$ and $\\ln(4) \\approx 1.386...$, I do not know if there is any significance to this at all).\n",
    "\n",
    "The **Tag 22** features also have a very interesting daily pattern. For example, here are scatter and cumulative plots over three days for feature 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_0 = train_data.loc[train_data['date'] == 0]\n",
    "day_1 = train_data.loc[train_data['date'] == 1]\n",
    "day_3 = train_data.loc[train_data['date'] == 3]\n",
    "three_days = pd.concat([day_0, day_1, day_3])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 6), sharex=True)\n",
    "ax[0].scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].set_ylabel('value')\n",
    "ax[0].set_title('feature_64 (days 0, 1 and 3)')\n",
    "ax[1].scatter(three_days.ts_id, pd.Series(three_days['feature_64']).cumsum(), s=0.5, color='r')\n",
    "ax[1].set_xlabel('ts_id')\n",
    "ax[1].set_ylabel('cumulative sum')\n",
    "ax[1].set_title('')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global minimum value of `feature_64` is \\\\( \\approx -6.4 \\\\) and the global maximum value is \\\\( \\approx 8 \\\\) (not all days reach these limits). It is curious that a trading day on the New York Stock Exchange spans from 9:30 until 16:00. What if the units of `feature_64` were \\\\( \\approx 30 \\\\) minutes, and `feature_64 = 0` corresponds to 12:00? Just for fun let us make a plot of the *arcsin* function, renaming the *y*-axis as the hours of the day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1,1,0.01) \n",
    "y = 2 * np.arcsin(x) +1\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.plot(x,y, lw=3)\n",
    "ax.set(xticklabels=[]) \n",
    "ax.set(yticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])  \n",
    "ax.set_title(\"2$\\it{arcsin}$(t) +1\", fontsize=18)\n",
    "ax.set_xlabel (\"'tick' time\", fontsize=18)\n",
    "ax.set_ylabel (\"Clock time\", fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, for some reason, the tick time is more frequent at the start and end of the day than in the middle. Also for fun let us plot the hypothetical tick frequency, *i.e.*\n",
    "\n",
    "$$ \\frac{d}{dt} (2 \\arcsin(t) +1) = \\frac{2}{\\sqrt{1-t^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dash = np.arange(-0.98,0.99,0.01) \n",
    "y_dash = 2 / np.sqrt(1-(x_dash**2))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.plot(x_dash,y_dash, lw=3)\n",
    "ax.set(yticklabels=[])\n",
    "ax.xaxis.set_ticks(np.arange(-1, 1, 0.28))\n",
    "#ax.set(xticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])\n",
    "ax.set(xticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])  \n",
    "ax.set_title(\"d/dt (2$\\it{arcsin}$(t) +1)\", fontsize=18)\n",
    "ax.set_xlabel (\"Clock time\", fontsize=18)\n",
    "ax.set_ylabel (\"'tick' frequency\", fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were so, then perhaps the period of missing values seen at the start of the day for some of the  features (see the section below on missing values) is actually similar to the period of missing values seen during the middle of the day? Also perhaps the higher tick frequency at the beginning and end of the day is due to a lot of buying when the day opens, and a lot of selling towards the close of the day so as to have no significant position overnight?\n",
    "\n",
    "It was first suggested (if I am not mistaken) by [marketneutral](https://www.kaggle.com/marketneutral) in a [post](https://www.kaggle.com/c/jane-street-market-prediction/discussion/201264#1101507) that the data *may* correspond to equities traded on the [Tokyo Stock Exchange](https://www.jpx.co.jp/english/derivatives/rules/trading-hours/index.html), whose trading hours are from 9:00 until 11:30, a break for lunch, and then from 12:30 until 15:00. This could explain the central discontinuity in the Tag 22 features.\n",
    "\n",
    "We shall now also look at `feature 65`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_days.plot.scatter(x='ts_id', y='feature_65', s=0.5, figsize=(15,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very interesting look at the Tag 22 features see the notebook [\"*Important and Hidden Temporal Data*\"](https://www.kaggle.com/lachlansuter/important-and-hidden-temporal-data) written by [Lachlan Suter](https://www.kaggle.com/lachlansuter).\n",
    "### 'Noisy' features\n",
    "* 3, 4, 5, 6\n",
    "* 8, 10, 12, 14, 16\n",
    "* 37, 38, 39, 40\n",
    "* 72, 73, 74, 75, 76\n",
    "* 78, 79, 80, 81, 82\n",
    "* 83\n",
    "\n",
    "Here are cumulative plots of some of these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(16,8))\n",
    "ax1.set_title (\"features 3 and 4 (+Tag 9)\", fontsize=18);\n",
    "ax1.plot((pd.Series(train_data['feature_3']).cumsum()), lw=2, color='blue')\n",
    "ax1.plot((pd.Series(train_data['feature_4']).cumsum()), lw=2, color='red')\n",
    "\n",
    "ax2.set_title (\"features 5 and 6 (+Tag 9)\", fontsize=18);\n",
    "ax2.plot((pd.Series(train_data['feature_5']).cumsum()), lw=2, color='blue')\n",
    "ax2.plot((pd.Series(train_data['feature_6']).cumsum()), lw=2, color='red')\n",
    "\n",
    "ax3.set_title (\"features 37 and 38 (+Tag 9)\", fontsize=18);\n",
    "ax3.plot((pd.Series(train_data['feature_37']).cumsum()), lw=2, color='blue')\n",
    "ax3.plot((pd.Series(train_data['feature_38']).cumsum()), lw=2, color='red')\n",
    "ax3.set_xlabel (\"Trade\", fontsize=18)\n",
    "\n",
    "ax4.set_title (\"features 39 and 40 (+Tag 9)\", fontsize=18);\n",
    "ax4.plot((pd.Series(train_data['feature_39']).cumsum()), lw=2, color='blue')\n",
    "ax4.plot((pd.Series(train_data['feature_40']).cumsum()), lw=2, color='red')\n",
    "ax4.set_xlabel (\"Trade\", fontsize=18)\n",
    "#ax4.axvline(x=514052, linestyle='--', alpha=0.3, c='black', lw=1)\n",
    "#ax4.axvspan(0,  514052, color=sns.xkcd_rgb['grey'], alpha=0.1);\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could these represent offer prices, and those with with **Tag 9** bid prices? That said, we can see that after day 85 the value of `feature_40` actually becomes greater than the value of `feature_39`.\n",
    "\n",
    "### `feature_51` (Tag 19)\n",
    "In the Topic [\"*Weight and feature_51 de-anonymized*\"](https://www.kaggle.com/c/jane-street-market-prediction/discussion/202014) by [marketneutral](https://www.kaggle.com/marketneutral) it is suggested that `feature_51` is the (log of) the average daily volume of the stock.\n",
    "Here I reproduce the plot of `feature_51` w.r.t. `weight` for non-zero weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax.scatter(train_data_nonZero.weight, train_data_nonZero.feature_51, s=0.1, color='b')\n",
    "ax.set_xlabel('weight')\n",
    "ax.set_ylabel('feature_51')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `feature_52` (Tag 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "feature_0 = pd.Series(train_data['feature_52']).cumsum()\n",
    "ax.set_xlabel (\"ts_id\", fontsize=18)\n",
    "ax.set_ylabel (\"feature_52 (cumulative)\", fontsize=12);\n",
    "feature_0.plot(lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having the following lag plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4, 4))\n",
    "lag_plot(day_0['feature_52'], s=0.5, ax=ax)\n",
    "ax.title.set_text('feature_52')\n",
    "ax.set_xlabel(\"ts_id (n)\")\n",
    "ax.set_ylabel(\"ts_id (n+1)\")\n",
    "ax.plot(0, 0, 'r.', markersize=15.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following curious relationship with `resp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax.scatter(train_data_nonZero.feature_52, train_data_nonZero.resp, s=0.1, color='b')\n",
    "ax.set_xlabel('feature_52')\n",
    "ax.set_ylabel('resp')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Negative' features\n",
    "Features 73, 75, 76, 77 (noisy), 79, 81(noisy), 82. These are all found in the **Tag 23** section.\n",
    "\n",
    "### 'Hybrid' features (Tag 21): \n",
    "55, 56, 57, 58, 59.\n",
    "\n",
    "These start off noisy, with prominent almost discontinuous steps around the 0.2M, 0.5M, and 0.8M trade marks, then go linear. These five features form the \"**Tag 21**\" set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "feature_55= pd.Series(train_data['feature_55']).cumsum()\n",
    "feature_56= pd.Series(train_data['feature_56']).cumsum()\n",
    "feature_57= pd.Series(train_data['feature_57']).cumsum()\n",
    "feature_58= pd.Series(train_data['feature_58']).cumsum()\n",
    "feature_59= pd.Series(train_data['feature_59']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_title (\"Cumulative plot for the 'Tag 21' features (55-59)\", fontsize=18)\n",
    "ax.axvline(x=514052, linestyle='--', alpha=0.3, c='black', lw=1)\n",
    "ax.axvspan(0,  514052, color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "feature_55.plot(lw=3)\n",
    "feature_56.plot(lw=3)\n",
    "feature_57.plot(lw=3)\n",
    "feature_58.plot(lw=3)\n",
    "feature_59.plot(lw=3)\n",
    "plt.legend(loc=\"upper left\");\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if these are associated with the five `resp` values? Perhaps: \n",
    "* `feature_55` is related to `resp_1`\n",
    "* `feature_56` is related to `resp_4` \n",
    "* `feature_57` is related to `resp_2` \n",
    "* `feature_58` is related to `resp_3` \n",
    "* `feature_59` is related to `resp`\n",
    "\n",
    "If that *is* the case then \n",
    "* **Tag 0** represents `resp_4` features\n",
    "* **Tag 1** represents `resp` features\n",
    "* **Tag 2** represents `resp_3` features\n",
    "* **Tag 3** represents `resp_2` features\n",
    "* **Tag 4** represents `resp_1` features\n",
    "\n",
    "*i.e.*\n",
    "* `resp_1` related features: 7, 8, 17, 18, 27, 28, 55, 72, 78, 84, 90, 96, 102, 108, 114, 120, and 121 <font color=\"red\">(Note: 79.6% of all of the missing data is found within this set of features).</font>\n",
    "* `resp_2` related features: 11, 12, 21, 22, 31, 32, 57, 74, 80, 86, 92, 98, 104, 110, 116, 124, and 125 <font color=\"red\">(Note: 15.2% of all of the missing data is found within this set of features).</font>\n",
    "* `resp_3` related features: 13, 14, 23, 24, 33, 34, 58, 75, 81, 87, 93, 99, 105, 111, 117, 126, and 127\n",
    "* `resp_4` related features: 9, 10, 19, 20, 29, 30, 56, 73, 79, 85, 91, 97, 103, 109, 115, 122, and 123\n",
    "* `resp` related features: 15, 16, 25, 26, 35, 36, 59, 76, 82, 88, 94, 100, 106, 112, 118, 128, and 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have made some t-distributed stochastic neighbor embedding (t-SNE) plots for some of the feature groups in a separate notebook [\"*Jane Street: t-SNE using RAPIDS cuML*\"](https://www.kaggle.com/carlmcbrideellis/jane-street-t-sne-using-rapids-cuml) as they take a long time to calculate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The features.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also provided with a `features.csv` file which contains \"*metadata pertaining to the anonymized features*\". Let us take a quick look at it, where `1` is `True` and `0` is `False`. The file has 29 \"tags\" associated with each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tags = pd.read_csv(\"../features.csv\" ,index_col=0)\n",
    "# convert to binary\n",
    "feature_tags = feature_tags*1\n",
    "# plot a transposed dataframe\n",
    "feature_tags.T.style.background_gradient(cmap='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun let us re-plot the above data, but now in '8-bit' mode; totally illegible, but may perhaps serve as an overall visual aid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,14))\n",
    "sns.heatmap(feature_tags.T,\n",
    "            cbar=False,\n",
    "            xticklabels=False,\n",
    "            yticklabels=False,\n",
    "            cmap=\"Oranges\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sum the number of tags for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sum = pd.DataFrame(feature_tags.T.sum(axis=0),columns=['Number of tags'])\n",
    "tag_sum.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all of the features have at least one tag, and some as many a four. All except, that is, for `feature_0`, which has no tags at all.\n",
    "There seem to be 5 (ish) regions that seem different to each other:\n",
    "\n",
    "| '*Region*' | features | Tags | <font color=\"red\">missing values?</font> | observations |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| 0 | feature_0 | none | none | -1 or +1 |\n",
    "| 1  | 1...6 | **Tag 6** |  |\n",
    "| 2  | 7-36  | **Tag 6** |  |\n",
    "| 2a |  7..16 | + 11 | <font color=\"red\">7, 8 and 11, 12</font> | |\n",
    "| 2b | 17...26 | + 12 | <font color=\"red\">17, 18 and 21, 22 </font>| |\n",
    "| 2c | 27...36 | + 13 | <font color=\"red\">27, 28 and 31, 32 </font>| |\n",
    "| 3  | 37...72 | various | |\n",
    "| 3a | 55...59 | Tag 21 | | All hybrid |\n",
    "| 3b | 60...68 | Tag 22 | | Clock + time features? |\n",
    "| 4  | 72-119 | **Tag 23** | |\n",
    "| 4a | 72...77  |  + 15 & 27 | <font color=\"red\"> 72 and 74</font> | |\n",
    "| 4b | 78...83  |  + 17 & 27 |<font color=\"red\"> 78 and 80</font>| |\n",
    "| 4c | 84...89  |  + 15 & 25 | <font color=\"red\"> 84 and 86</font>| |\n",
    "| 4d | 90...95  |  + 17 & 25 | <font color=\"red\"> 90 and 92</font>| |\n",
    "| 4e | 96...101 |  + 15 & 24 | <font color=\"red\"> 96 and 98</font>| |\n",
    "| 4f | 102...107 | + 17 & 24 | <font color=\"red\"> 102 and 104</font>| |\n",
    "| 4g | 108...113 | + 15 & 26 | <font color=\"red\"> 108 and 110</font>| |\n",
    "| 4h | 114...119 | + 17 & 26 | <font color=\"red\"> 114 and 116</font>| |\n",
    "| 5  | 120...129 |**Tag 28** | |\n",
    "| 5a | 120 |+ 4      | <font color=\"red\">missing data</font> | |\n",
    "| 5b | 121 |+ 4 & 16 |<font color=\"red\">missing data</font> | |\n",
    "| 5c | 122 |+ 0|| |\n",
    "| 5d | 123 |+ 0 & 16|| |\n",
    "| 5e | 124 |+ 3|| |\n",
    "| 5f | 125 |+ 3 & 16|| |\n",
    "| 5g | 126 |+ 2|| |\n",
    "| 5h | 127 |+ 2 & 16|| |\n",
    "| 5i | 128 |+ 1|| |\n",
    "| 5j | 129 |+ 1 & 16|||\n",
    "\n",
    "\n",
    "Here, more than anywhere, merits the classic: \"*To be continued...*\". \n",
    "\n",
    "It is very difficult to unravel what is going on in a traditional tabular format, and [quillio](https://www.kaggle.com/quillio) has written a very interesting notebook [\"🌐*EDA: Tag Network Analysis (networkx + gephi)*🌐\"](https://www.kaggle.com/quillio/eda-tag-network-analysis-networkx-gephi) in which a graph analysis is performed showing relationships between the tags and the features.\n",
    "\n",
    "Also, [Greg Calvez](https://www.kaggle.com/gregorycalvez) has produced an excellent series of notebooks dedicated to understanding the tags:\n",
    "\n",
    "* [\"*De-anonymization: Time Aggregation Tags*\"](https://www.kaggle.com/gregorycalvez/de-anonymization-time-aggregation-tags), looking at `tag_{0, 1, 2, 3, 4, 5}`\n",
    "* [\"*De-anonymization: Price, Quantity, Stocks*\"](https://www.kaggle.com/gregorycalvez/de-anonymization-price-quantity-stocks), suggestinging that `tag_6` is related to prices, `tag_23` is realted to volume, and `tag_20` could be related to spread\n",
    "* [\"*De-anonymization: Min, Max and Time*\"](https://www.kaggle.com/gregorycalvez/de-anonymization-min-max-and-time), suggestinging that `tag_12` is related to minima, `tag_13` is related to maxima, and `tag_22` is related to time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target of this competition is the `action`: 1 to make the trade and 0 to pass on it. In view of this let us add a new 'binary' column to our test dataset called `action` such that if `resp` is positive then `action=1` else `action=0`, *i.e.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['action'] = ((train_data['resp'])>0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the overall action to inaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_action_sum   = train_data['action'].groupby(train_data['date']).sum()\n",
    "daily_action_count = train_data['action'].groupby(train_data['date']).count()\n",
    "daily_ratio        = daily_action_sum/daily_action_count\n",
    "# now plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.plot(daily_ratio)\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_ylabel (\"ratio\", fontsize=18)\n",
    "ax.set_title (\"Daily ratio of action to inaction\", fontsize=18)\n",
    "plt.axhline(0.5, linestyle='--', alpha=0.85, c='r');\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_xlim(xmax=500)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that with the above formula overall we are very slightly more proactive (0.4%) than inactive. How does this look daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_ratio_mean = daily_ratio.mean()\n",
    "print('The mean daily ratio is %.3f' % daily_ratio_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_ratio_max = daily_ratio.max()\n",
    "print('The maximum daily ratio is %.3f' % daily_ratio_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which occurred on day **294** (we shall hear more about day 294 in the missing data section below).\n",
    "\n",
    "Obviously the above is a very simplistic target for such a complex set of data. For a more detailed look my I suggest the notebook [\"*Target Engineering; CV; ⚡ Multi-Target*\"](https://www.kaggle.com/marketneutral/target-engineering-cv-multi-target) written by [marketneutral](https://www.kaggle.com/marketneutral), where other options are explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let us take a look at the first day (\"day 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we shall make a new dataframe called `day_0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_0 = train_data.loc[train_data['date'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "balance= pd.Series(day_0['resp']).cumsum()\n",
    "resp_1= pd.Series(day_0['resp_1']).cumsum()\n",
    "resp_2= pd.Series(day_0['resp_2']).cumsum()\n",
    "resp_3= pd.Series(day_0['resp_3']).cumsum()\n",
    "resp_4= pd.Series(day_0['resp_4']).cumsum()\n",
    "ax.set_xlabel (\"Trade\", fontsize=18)\n",
    "ax.set_title (\"Cumulative values for resp and time horizons 1, 2, 3, and 4 for day 0\", fontsize=18)\n",
    "balance.plot(lw=3)\n",
    "resp_1.plot(lw=3)\n",
    "resp_2.plot(lw=3)\n",
    "resp_3.plot(lw=3)\n",
    "resp_4.plot(lw=3)\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics of the `train.csv` file for day 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple [descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics) of the day 0 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there any missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with let us look at day 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(day_0, color=(0.35, 0.35, 0.75));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphically we can see that indeed there are chunks of missing data (in white) in some of the columns, and there appears to be a pattern. Let us take a look only at `feature_7` (the first `resp_1` feature) and `feature_11` (the first `resp_2` feature): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_7_11 = day_0.iloc[:, [14,18]]\n",
    "msno.matrix(feats_7_11, color=(0.35, 0.35, 0.75), width_ratios=(1, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the missing data does not appear to be random, indeed there appear to be two big chunks missing at the start and in the middle of each column. Let us assume that a [trading day](https://en.wikipedia.org/wiki/Trading_day) spans from 9:30 until 16:00. Let us also assume that the trades occur at regular intervals (which is almost certainly *not* the case) then `feature_7` has chunks of missing data from 9:30 until 10:03, and is missing ≈16 minutes from 13:17 until 13:33. `feature_11` has missing data from 9:30 until 9:35, and is missing ≈5½ minutes from 13:17 until 13:22.\n",
    "\n",
    "Now let us look at the sum of the number of missing data in each column for the whole `train.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_data = pd.DataFrame(train_data.isna().sum().sort_values(ascending=False),columns=['Total missing'])\n",
    "#missing_data.T\n",
    "\n",
    "gone = train_data.isnull().sum()\n",
    "px.bar(gone, color=gone.values, title=\"Total number of missing values for each column\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, \n",
    "\n",
    "* **79.6%** of all the missing data is located in the **Tag 4** group, which represent the `resp_1` features\n",
    "* **15.2%** of the missing data is in the **Tag 3** group, which represent the `resp_2` features\n",
    "* In total, the features associated with `resp_1` and `resp_2` make up **> 95%** of all the missing data.\n",
    "\n",
    "We can see that features 7 and 8 both have exactly the same number of missing values (393135). \n",
    "17 and 18, and 27 and 28 all have 395535 missing values each. These are all `resp_1` features.\n",
    "\n",
    "Next we have features 72, 78, 84, 90, 96, 102, 108, 114 all with 351426 missing values each. These too are all `resp_1` features.\n",
    "\n",
    "Features 21, 22, 31, 32 have 81444 missing values, closely followed by features 11 and 12. These are all `resp_2` features.\n",
    "\n",
    "There are more features with even less missing values. I think the interesting thing is not so much the quantity of missing values in so much as it may tell us which features represent similar measures/metrics.\n",
    "\n",
    "Is day 0 special, or does every day have missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_features = train_data.iloc[:,7:137].isnull().sum(axis=1).groupby(train_data['date']).sum().to_frame()\n",
    "# now make a plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.plot(missing_features)\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_title (\"Total number of missing values in all features for each day\", fontsize=18)\n",
    "ax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=2)\n",
    "ax.axvspan(0,  85, color=sns.xkcd_rgb['grey'], alpha=0.1)\n",
    "ax.set_xlim(xmin=0)\n",
    "ax.set_xlim(xmax=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we can see that there is missing data *almost* every day, with no discernible pattern (weekly, monthly, *etc*). The exceptions are days **2** and **294**, which we shall look at in the next section.\n",
    "\n",
    "In the notebook [\"*Jane Street EDA Market Regime*\"](https://www.kaggle.com/marketneutral/jane-street-eda-market-regime) written by [marketneutral](https://www.kaggle.com/marketneutral) a plot is made of the number of trades per day, and is strikingly similar to the above plot. In view of this, for curiosity, we shall plot the number of missing values in the features with respect to the number of trades, for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_weights    = train_data[['date', 'weight']].groupby('date').agg(['count'])\n",
    "result = pd.merge(count_weights, missing_features, on = \"date\", how = \"inner\")\n",
    "result.columns = ['weights','missing']\n",
    "result['ratio'] = result['missing']/result['weights']\n",
    "missing_per_trade = result['ratio'].mean()\n",
    "\n",
    "# now make a plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plt.plot(result['ratio'])\n",
    "plt.axhline(missing_per_trade, linestyle='--', alpha=0.85, c='r');\n",
    "ax.set_xlabel (\"Day\", fontsize=18)\n",
    "ax.set_title (\"Average number of missing feature values per trade, for each day\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on average there are $\\approx$ 3 missing feature values per trade, per day, except for two spikes located on days 2 and 294 where there are no missing values at all. (The most missing values are on day 14).\n",
    "\n",
    "This raises the question of [what to do with missing data in the unseen test data?](https://www.kaggle.com/c/jane-street-market-prediction/discussion/200691). Whatever one decides to do, in this competition time is of the essence, so we have to do it fast, and [Yirun Zhang](https://www.kaggle.com/gogo827jz) has made an exhaustive study of the time taken in various filling methods in the notebook [\"*Optimise Speed of Filling-NaN Function*\"](https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there any missing data: Days 2 and 294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we produce scatter plots of `feature_64` we see that each day has the same sweeping pattern. However we see that **day 2** has only 231 `ts_id`  which all seem to originate from the very end of the day. Here is a plot of day 1 (in blue), day 2 (in red) and day 3 (blue again). Day 2 has been encircled as a visual aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_1 = train_data.loc[train_data['date'] == 1]\n",
    "day_2 = train_data.loc[train_data['date'] == 2]\n",
    "day_3 = train_data.loc[train_data['date'] == 3]\n",
    "three_days = pd.concat([day_1, day_2, day_3])\n",
    "\n",
    "#td = three_days.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='blue')\n",
    "#day_2.plot.scatter(x='ts_id', y='feature_64', s=0.5, figsize=(15,4), color='red', ax=td);\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "ax.scatter(three_days.ts_id, three_days.feature_64, s=0.5, color='b')\n",
    "ax.scatter(day_2.ts_id, day_2.feature_64, s=0.5, color='r')\n",
    "ax.scatter(15150, 5.2, s=1800, facecolors='none', edgecolors='black', linestyle='--', lw=2)\n",
    "ax.set_xlabel('feature_64')\n",
    "ax.set_ylabel('ts_id')\n",
    "ax.set_title('feature_64 for days 1, 2 and 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for **day 294**, which has only 29 `ts_id`.\n",
    "This would also explain why days 2 and 294 have none of the missing values that we usually find during breakfast and lunch the other days.\n",
    "It is possibly worth treating these two days as outliers and dropping them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there any correlation between day 100 and day 200?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the days independent? For the moment let us take a look at day(100) and day(200) using a [Pearson pairwise correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) matrix (this is a **big** matrix!). Why days 100 and 200? Because they are far apart in time, thus reducing any temporal leakage.\n",
    "We shall use a [diverging colormap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html) where red indicates positive linear correlation, and blue indicates linear anti-correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_100  = train_data.loc[train_data['date'] == 100]\n",
    "day_200  = train_data.loc[train_data['date'] == 200]\n",
    "day_100_and_200 = pd.concat([day_100, day_200])\n",
    "day_100_and_200.corr(method='pearson').style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a correlation of only 0.54 between our simple definition of `action` and the value of `resp`. It is interesting to note that no one single feature shows a strong correlation to `resp`.\n",
    "\n",
    "It has been mentioned that the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) is more apropo for financial data. One can simply change `method='pearson'` to `method='spearman'` in the [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to see such correlations.\n",
    "\n",
    "### Tag 28 section\n",
    "Navigating around there do appear to be some rather curious regions, for example between features 120 through to 129:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = day_100_and_200[[\"feature_120\",\"feature_121\",\"feature_122\",\"feature_123\",\"feature_124\",\"feature_125\",\"feature_126\",\"feature_127\",\"feature_128\",\"feature_129\"]]\n",
    "subset.corr(method='pearson').style.background_gradient(cmap='coolwarm', low=1, high=0, axis=None).set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all **Tag 28** features. \n",
    "* `feature_120` and `feature_121` both also have **Tag 4** which I suggest are related to `resp_1`\n",
    "* `feature_122` and `feature_123` both also have **Tag 0** which I suggest are related to `resp_4`\n",
    "* `feature_124` and `feature_125` both also have **Tag 3** which I suggest are related to `resp_2`\n",
    "* `feature_126` and `feature_127` both also have **Tag 2** which I suggest are related to `resp_3`\n",
    "* `feature_128` and `feature_129` both also have **Tag 1** which I suggest are related to `resp`\n",
    "\n",
    "from this we can see that (for days 100 and 200) for the Tag 28 features between\n",
    "* `resp` and `resp_4` there is a linear correlation of 0.98\n",
    "* `resp` and `resp_3` there is a linear correlation of 0.97\n",
    "* `resp` and `resp_2` there is a linear correlation of 0.94\n",
    "* `resp` and `resp_1` there is a linear correlation of 0.89\n",
    "\n",
    "\n",
    "\n",
    "### High correlations\n",
    "We shall now find the pairs of features with a correlation > |0.992|:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_day_100   = day_100.iloc[:,7:137]\n",
    "features_day_200   = day_200.iloc[:,7:137]\n",
    "features_100_and_200 = pd.concat([features_day_100, features_day_200])\n",
    "\n",
    "# code from: https://izziswift.com/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas/\n",
    "def corrFilter(x: pd.DataFrame, bound: float):\n",
    "    xCorr = x.corr()\n",
    "    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n",
    "    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n",
    "    return xFlattened\n",
    "\n",
    "corrFilter(features_100_and_200, .992).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we can certainly cut down on the number of eventual features in our model. A good place to start will be to look at the features in the region of the **60's** (*i.e.* **Tag 22**) as we have seen before there seems to be a lot of correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wall time taken for a submission to return a score in this competition is around 3½ hours, so testing a script before submitting is (as always) *very* important. We are provided with some test data in the smaller (36 MB) file `example_test.csv`. This file contains over 15k rows, and covers three days of data. It consists of the 130 `features` as in the `train.csv`, as well as the `weight` for each trade. It is interesting to note that `example_test.csv` contains no `resp` data.\n",
    "\n",
    "The three days of data in the test file correspond to day 0, day 1 and day 2. Note that day 2 only contains data from the very end of the day, as with day 2 in the `train.csv` file, so should be treated with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
